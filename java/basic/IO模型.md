# IO
## 用户态与内核态
普通的程序代码是运行在**用户空间**的，操作系统的代码运行在**内核空间**，用户空间无法直接访问内核空间。当一个进程运行在用户空间时就处于用户态，运行在内核空间就属于内核态。

当处于用户空间的需要进行系统调用时，需要调用操作系统提供的API，这就好比RPC调用，总归是有一定开销的。

当程序获取网络数据时，会有两次拷贝：

1. 程序等待数据从网卡拷贝到内核空间
2. 程序等待数据从内核空间拷贝到用户空间，用户态进程才可以访问数据

## 同步阻塞IO
当用户线程需要获取网络数据时，会调用操作系统的read函数，此时用户线程等待网卡数据拷贝到内核，内核拷贝到用户空间，这整过过程中，线程一直被阻塞等待。该模型的缺点很明显，同步阻塞效率很低；一个线程处理一个连接，线程的开销属于重资源。

## 同步非阻塞IO
基于同步阻塞IO的缺点，同步非阻塞进行了一些优化，线程在请求数据后不再阻塞，而是返回错误。缺点就是：有海量线程进行系统调用，上下文切换频繁。

## IO多路复用
相比于同步非阻塞IO的线程数不可控的缺点，可以使用一个线程查看多个连接是否数据已经准备就绪，落实到模型当中就是select，我们可以向select中注册需要监听的连接，如果有连接的数据已经准备好，准备好就可以read，这样就实现了少量线程监听多个连接，降低的上下文切换的频率。

IO多路复用是目前用的最多的IO技术
在接收到每一个客户端的连接后，将文件描述符（connfd）加入一个集合中，开一个新的线程去遍历数据，调用read方法。返回-1就是没有数据准备好，但是每次请求read也是一笔不小的开销，所以如果循环read的这个过程交给操作系统做，我们将所有的文件描述符传给操作系统，这样就会解决频繁调用read的问题。
**select**函数是操作系统提供的调用函数，可以通过它将文件描述符传给操作系统，select函数返回后，操作系统会将准备就绪的文件做上标记

可以发现几个问题：
1. select函数的文件描述符需要拷贝一份到内核，在高并发下这个拷贝的资源消耗也是惊人的。
2. select在内核层仍然是通过同步的方法检查文件描述符
3. select返回的仅仅是就绪状态的文件描述符数量，具体哪个可读需要用户自己遍历

### poll
poll是在select的基础上取消了只能监听1024个文件描述符的限制

### epoll
epoll模型解决了许多痛点
1. 内核中只保存一份文件描述符集合
2. 不再轮询方式找到就绪的文件描述符，通过异步事件通知
3. epoll会将完成的IO事件的文件描述符返回给用户，用户无需遍历整个集合。
## 信号驱动式IO
上面的所有模型中，都需要时刻主动查询数据是否已经准备就绪。信号驱动式IO就是不需要我们主动查询结果，而是数据准备好后主动推送给我们。但是信号驱动式IO用的很少，主要原因是TCP协议的socket可以产生的信号事件有多种，无法分辨是哪种事件的信号。

## 异步IO
上面的模型中，最后一步read调用还是需要阻塞，所以实现真正的异步，需要把这一步的阻塞也去掉。思路比较简单，内核帮我们把数据从网卡拷贝到用户空间后，再告知用户线程数据准备就绪。AIO的实现就是调用aio_read，全程没有阻塞点。
但是AIO不常见，Linux对不支持AIO，而支持AIO的都是使用epoll模型实现的。win是支持AIO的，不过主流的服务器都是Linux，所以用的最多的还是IO多路复用